{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_Part4",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "P2cO1gr22RrA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LDA on Google Collab\n"
      ]
    },
    {
      "metadata": {
        "id": "mvV3Duay6HgQ",
        "colab_type": "code",
        "outputId": "9dd255af-cc27-42ff-de2e-e3c027a37dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 213.4MB 90kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 27.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cd/54/c2/abfcc942eddeaa7101228ebd6127a30dbdf903c72db4235b23\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fKbZfq1G7YWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RFdedXfJ7a6l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pyspark as ps\n",
        "import warnings\n",
        "from pyspark.sql import SQLContext\n",
        "  \n",
        "sc = ps.SparkContext('local[4]')\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvZod-yOCqhK",
        "colab_type": "code",
        "outputId": "6f564a0a-d203-4796-e35d-245ddec02505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1390
        }
      },
      "cell_type": "code",
      "source": [
        " !unzip cookbook_text.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cookbook_text.zip\n",
            "   creating: cookbook_text/\n",
            "  inflating: cookbook_text/amem.txt  \n",
            "  inflating: cookbook_text/amwh.txt  \n",
            "  inflating: cookbook_text/army.txt  \n",
            "  inflating: cookbook_text/aunt.txt  \n",
            "  inflating: cookbook_text/bart.txt  \n",
            "  inflating: cookbook_text/beec.txt  \n",
            "  inflating: cookbook_text/blue.txt  \n",
            "  inflating: cookbook_text/bost.txt  \n",
            "  inflating: cookbook_text/brkf.txt  \n",
            "  inflating: cookbook_text/buck.txt  \n",
            "  inflating: cookbook_text/cclu.txt  \n",
            "  inflating: cookbook_text/chas.txt  \n",
            "  inflating: cookbook_text/chin.txt  \n",
            "  inflating: cookbook_text/choc.txt  \n",
            "  inflating: cookbook_text/comm.txt  \n",
            "  inflating: cookbook_text/conf.txt  \n",
            "  inflating: cookbook_text/coow.txt  \n",
            "  inflating: cookbook_text/creo.txt  \n",
            "  inflating: cookbook_text/dcvb.txt  \n",
            "  inflating: cookbook_text/dish.txt  \n",
            "  inflating: cookbook_text/dome.txt  \n",
            "  inflating: cookbook_text/econ.txt  \n",
            "  inflating: cookbook_text/ency.txt  \n",
            "  inflating: cookbook_text/engl.txt  \n",
            "  inflating: cookbook_text/epia.txt  \n",
            "  inflating: cookbook_text/epib.txt  \n",
            "  inflating: cookbook_text/favd.txt  \n",
            "  inflating: cookbook_text/fcsc.txt  \n",
            "  inflating: cookbook_text/fish.txt  \n",
            "  inflating: cookbook_text/fofb.txt  \n",
            "  inflating: cookbook_text/fore.txt  \n",
            "  inflating: cookbook_text/fran.txt  \n",
            "  inflating: cookbook_text/frca.txt  \n",
            "  inflating: cookbook_text/frch.txt  \n",
            "  inflating: cookbook_text/gohk.txt  \n",
            "  inflating: cookbook_text/good.txt  \n",
            "  inflating: cookbook_text/grea.txt  \n",
            "  inflating: cookbook_text/gtte.txt  \n",
            "  inflating: cookbook_text/hand.txt  \n",
            "  inflating: cookbook_text/henr.txt  \n",
            "  inflating: cookbook_text/hosf.txt  \n",
            "  inflating: cookbook_text/hote.txt  \n",
            "  inflating: cookbook_text/hous.txt  \n",
            "  inflating: cookbook_text/ital.txt  \n",
            "  inflating: cookbook_text/jenn.txt  \n",
            "  inflating: cookbook_text/jewi.txt  \n",
            "  inflating: cookbook_text/lady.txt  \n",
            "  inflating: cookbook_text/ldnw.txt  \n",
            "  inflating: cookbook_text/linc.txt  \n",
            "  inflating: cookbook_text/mara.txt  \n",
            "  inflating: cookbook_text/mary.txt  \n",
            "  inflating: cookbook_text/matf.txt  \n",
            "  inflating: cookbook_text/miss.txt  \n",
            "  inflating: cookbook_text/neig.txt  \n",
            "  inflating: cookbook_text/notm.txt  \n",
            "  inflating: cookbook_text/oldv.txt  \n",
            "  inflating: cookbook_text/orie.txt  \n",
            "  inflating: cookbook_text/pach.txt  \n",
            "  inflating: cookbook_text/pcdg.txt  \n",
            "  inflating: cookbook_text/prac.txt  \n",
            "  inflating: cookbook_text/pres.txt  \n",
            "  inflating: cookbook_text/prho.txt  \n",
            "  inflating: cookbook_text/rore.txt  \n",
            "  inflating: cookbook_text/sauc.txt  \n",
            "  inflating: cookbook_text/scie.txt  \n",
            "  inflating: cookbook_text/sett.txt  \n",
            "  inflating: cookbook_text/sevf.txt  \n",
            "  inflating: cookbook_text/swed.txt  \n",
            "  inflating: cookbook_text/syst.txt  \n",
            "  inflating: cookbook_text/time.txt  \n",
            "  inflating: cookbook_text/virg.txt  \n",
            "  inflating: cookbook_text/wash.txt  \n",
            "  inflating: cookbook_text/whit.txt  \n",
            "  inflating: cookbook_text/wosu.txt  \n",
            "  inflating: cookbook_text/youn.txt  \n",
            "  inflating: cookbook_text/zuni.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j9f-nkYn7vWL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext, Row\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.mllib.clustering import LDA, LDAModel\n",
        "from pyspark.mllib.linalg import Vector, Vectors\n",
        "\n",
        "#for path in cookbook_text:\n",
        "path = \"cookbook_text\"\n",
        "\n",
        "data = sc.textFile(path).zipWithIndex().map(lambda wordsID: Row(word_id = wordsID[1], words = wordsID[0].split(\" \")))\n",
        "docDF = sqlContext.createDataFrame(data)\n",
        "Vector1 = CountVectorizer(inputCol=\"words\", outputCol=\"vectors\",)\n",
        "model = Vector1.fit(docDF)\n",
        "result = model.transform(docDF)\n",
        "\n",
        "corpus = result.select(\"word_id\", \"vectors\").rdd.map(lambda z: [z[0],Vectors.fromML(z[1])]).cache()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BxxdkqAszVD5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cluster the documents into five distinct topics\n",
        "#Used LDA Model to classify text using default conditions\n",
        "lda_model = LDA.train(corpus, k=5,maxIterations=100,optimizer='online')\n",
        "topics = lda_model.topicsMatrix()\n",
        "vocabArray = model.vocabulary\n",
        "\n",
        "\n",
        "# number of words per topic\n",
        "wordNumbers = 15  \n",
        "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SHmll0hQC3zG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# specifing vector id of words to the actual words in the file\n",
        "\n",
        "def render(selected_topic):  \n",
        "    terms = selected_topic[0]\n",
        "    result = []\n",
        "    for i in range(wordNumbers):\n",
        "        term = vocabArray[terms[i]]\n",
        "        result.append(term)\n",
        "    return result\n",
        "\n",
        "#calling the render function\n",
        "final_topics = topicIndices.map(lambda topic: render(topic)).collect()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mSBknfz9zabQ",
        "colab_type": "code",
        "outputId": "9df01cef-0d5f-4e61-f34d-0ff34d1f3ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1601
        }
      },
      "cell_type": "code",
      "source": [
        "for topics in range(len(final_topics)):\n",
        "    print (\"Topic\" + str(topics) + \":\")\n",
        "    for phrase in final_topics[topics]:\n",
        "        print (phrase)\n",
        "    print ('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic0:\n",
            "M.\n",
            "E.\n",
            "A.\n",
            "of\n",
            "B.\n",
            "S.\n",
            "C.\n",
            "J.\n",
            "H.\n",
            "Red\n",
            "and\n",
            "D.\n",
            "W.\n",
            "L.\n",
            "CHAPTER\n",
            "\n",
            "\n",
            "Topic1:\n",
            "\n",
            "&#160;\n",
            "-\n",
            "&#224;\n",
            "la\n",
            "--\n",
            "de\n",
            "\"\n",
            "OF\n",
            "AND\n",
            "Cream\n",
            "No.\n",
            "of,\n",
            "THE\n",
            "au\n",
            "\n",
            "\n",
            "Topic2:\n",
            "\n",
            "the\n",
            "and\n",
            "of\n",
            "a\n",
            "in\n",
            "with\n",
            "it\n",
            "to\n",
            "or\n",
            ".\n",
            "one\n",
            "them\n",
            "is\n",
            "into\n",
            "\n",
            "\n",
            "Topic3:\n",
            "the\n",
            "of\n",
            "and\n",
            "to\n",
            "is\n",
            "in\n",
            "a\n",
            "be\n",
            "as\n",
            "are\n",
            "that\n",
            "for\n",
            "or\n",
            "it\n",
            "by\n",
            "\n",
            "\n",
            "Topic4:\n",
            "de\n",
            "\n",
            "MRS.\n",
            "SAUCE\n",
            "TO\n",
            "OR\n",
            "AND\n",
            "WITH\n",
            "et\n",
            "en\n",
            "FOR\n",
            "CHICKEN\n",
            "la\n",
            "DE\n",
            "au\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XTzeDQ-Dt8Ak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}